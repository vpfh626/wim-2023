{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIM Workshop: API-Webscraping with Python\n",
    "\n",
    "* Date: Nov 3, 2023\n",
    "* Instructor: Eehyun Kim (eehkim@iu.edu), Anne Kavalerchik (akavaler@iu.edu)\n",
    "\n",
    "## Example 1. Famous Quotes\n",
    "\n",
    "Let's open this link for our first practice: http://quotes.toscrape.com/. It's a website with quotations, the people they are attributed to, and the short biographies of those people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Structure of Website\n",
    "\n",
    "Click `setting (three vertical dots) > More Tools > Developer Tools` to find out the information about websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the packages we will use, which are basically the same with what we have used for APIs. We will use the python `requests` library to send HTTP requests and `BeautifulSoup` to extract the elements we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Response [200]>` means that our request was successful.\n",
    "Usually what we want is the text from a website.\n",
    "Let's get the text and print it. [Compare it to the source code of the actual webpage](view-source:http://quotes.toscrape.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmltext = response.text\n",
    "print(htmltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a combination of regular expressions, string matching, and loops to navigate the html, but luckily the Beautiful Soup package makes it much easier. [BeautifulSoup documentation is here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(htmltext,'html.parser')\n",
    "#print(soup) # this doesn't look much different than before we parsed it, but it will let us navigate it easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to navigate the website. Try to find your element of interest, in this case, first quote from Einstein and get the information of it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"quote\" itemscope=\"\" itemtype=\"http://schema.org/CreativeWork\">\n",
    "    <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\"> \n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above is shown like this:\n",
    "***\n",
    "<div class=\"quote\" itemscope=\"\" itemtype=\"http://schema.org/CreativeWork\">\n",
    "    <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\"> \n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `.find` with the target element and search it by the given attibute. For instance, html tag will be `div` with `class` of `quote`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(\"div\", {\"class\": \"quote\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, find the inner element under the nested tag structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quote = soup.find(\"div\", {\"class\": \"quote\"})\n",
    "quote_text1 = first_quote.find(\"span\", {\"itemprop\": \"text\"})\n",
    "quote_text2 = first_quote.find(\"span\", {\"class\": \"text\"})\n",
    "\n",
    "print(quote_text1)\n",
    "print(quote_text2)\n",
    "print(first_quote.span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quote_text2.text)\n",
    "print(quote_text2.get_text())\n",
    "print(first_quote.span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More efficiently, you can just write a line of code to retrieve the quote.\n",
    "\n",
    "print(soup.find(\"div\", {\"class\": \"quote\"}).find(\"span\", {\"itemprop\": \"text\"}).text)\n",
    "print(soup.find(\"div\", {\"class\": \"quote\"}).find(\"span\", {\"class\": \"text\"}).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Let's retrieve `author`, Albert Einstein, using `soup.find()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", {\"class\": \"quote\"}).find(\"small\", {\"itemprop\": \"author\"}).text\n",
    "soup.find(\"div\", {\"class\": \"quote\"}).find(\"small\", {\"class\": \"author\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must realize that there are multiple search terms that produce identical results! Try to find the best search term that works for you.\n",
    "\n",
    "While `find` returns only one, first appearing element, `.findAll` and `.select` return __all__ elements fitting those attributes. Let's get all of the tags for that quotation and use `get_text` to get __only__ the text from each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quote = soup.find(\"div\", {\"class\": \"quote\"})\n",
    "\n",
    "tags = first_quote.findAll(\"a\", {\"class\": \"tag\"})\n",
    "tags_list = []\n",
    "for tag in tags:\n",
    "    print(tag.get_text())\n",
    "    tags_list.append(tag.get_text())\n",
    "tags_list\n",
    "\n",
    "# We can do the equivalent task without a loop using this line:\n",
    "tags_list = [tag.get_text() for tag in tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all people using for loops\n",
    "\n",
    "Then, let's make a list of every person on this page, and then every quotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quotes = soup.findAll('div', {'class':'quote'})\n",
    "\n",
    "for quote in all_quotes:\n",
    "    # author\n",
    "    print(\"Author:\", quote.small.text)\n",
    "    # quote\n",
    "    print(\"Quote:\", quote.span.text)\n",
    "    # tags\n",
    "    tags = quote.findAll(\"a\", {\"class\": \"tag\"})\n",
    "    print(\"Tags:\", \", \".join([tag.text for tag in tags]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Review\n",
    "\n",
    "Great! Then let's review all the process. Make a function to collect every person/quote on the page and return a __list__ of information when a link is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_quotes(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    htmltext = response.text\n",
    "    soup = bs(htmltext,'html.parser')\n",
    "    \n",
    "    refined_list = []\n",
    "    all_quotes = soup.findAll('div', {'class':'quote'})\n",
    "\n",
    "    for quote in all_quotes:\n",
    "        quote_author = quote.small.text\n",
    "        quote_text = quote.span.text\n",
    "        quote_tags = quote.findAll(\"a\", {\"class\": \"tag\"})\n",
    "        tags_str = \", \".join([tag.text for tag in quote_tags])\n",
    "        refined_list.append([quote_author, quote_text, tags_str])\n",
    "    \n",
    "    return refined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://quotes.toscrape.com/\"\n",
    "\n",
    "result = list_quotes(url)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we __really__ want is a list of __every person on this website__. To do this, we need to use `requests` to call on all the pages.\n",
    "\n",
    "It's helpful to do some investigating first. Notice that [quotes.toscrape.com/page/1/](quotes.toscrape.com/page/1/) is this page we have been working with, [quotes.toscrape.com/page/2/](quotes.toscrape.com/page/2/) is the next page, and [quotes.toscrape.com/page/10/](quotes.toscrape.com/page/10/) is the last page. So our goal is to scrape these __10__ pages.\n",
    "\n",
    "We can generate these 10 different URLs. Then, we are basically going to repeat the process that we did to get all the information from the first page for all 10 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://quotes.toscrape.com/page/'\n",
    "\n",
    "all_quote_list = []\n",
    "\n",
    "for page_num in range(1, 11):\n",
    "    page_link = url + str(page_num)\n",
    "    print(page_link)\n",
    "    all_quote_list.extend(list_quotes(page_link))\n",
    "\n",
    "print(\"Number of Quotes:\", len(all_quote_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it! Here is the data we scraped. Let's use `pandas` and look at the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_df = pd.DataFrame(all_quote_list)\n",
    "quote_df = quote_df.rename(columns={0: \"Author\", 1: \"Quote\", 2: \"Tags\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this a JSON like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the `pandas` DataFrame as an Excel or CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_df.to_csv('all_quotes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice using a real life example. BillBoard Hot 100\n",
    "\n",
    "Suppose we are interested in Billboard Hot 100 and scrape song titles and performers from this link: https://www.billboard.com/charts/hot-100/ <br>\n",
    "__NOTE__: The structure of this site is much more complicated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.billboard.com/charts/hot-100/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "htmltext = response.text\n",
    "soup = bs(htmltext,'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_titles = soup.select(\"li.o-chart-results-list__item > h3#title-of-a-story\")\n",
    "performers = soup.select(\"li.o-chart-results-list__item > span.a-no-trucate\")\n",
    "\n",
    "print(len(song_titles))\n",
    "print(len(performers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_refined = [title.text.strip() for title in song_titles]\n",
    "performer_refined = [performer.text.strip() for performer in performers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df = pd.DataFrame({\"Song\": title_refined, \"Performer\": performer_refined})\n",
    "chart_df.index = range(1, 101)\n",
    "chart_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
